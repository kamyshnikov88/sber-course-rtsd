{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from shutil import copyfile\ncopyfile(src=\"../input/augmented-rsdd/engine.py\", dst=\"../working/engine.py\")\ncopyfile(src=\"../input/augmented-rsdd/coco_eval.py\", dst=\"../working/coco_eval.py\")\ncopyfile(src=\"../input/augmented-rsdd/coco_utils.py\", dst=\"../working/coco_utils.py\")\ncopyfile(src=\"../input/augmented-rsdd/group_by_aspect_ratio.py\", dst=\"../working/group_by_aspect_ratio.py\")\ncopyfile(src=\"../input/augmented-rsdd/presets.py\", dst=\"../working/presets.py\")\ncopyfile(src=\"../input/augmented-rsdd/train.py\", dst=\"../working/train.py\")\ncopyfile(src=\"../input/augmented-rsdd/transforms.py\", dst=\"../working/transforms.py\")\ncopyfile(src=\"../input/augmented-rsdd/utils.py\", dst=\"../working/utils.py\")\n\n# reinstall to fix \"module 'torch.optim.lr_scheduler'\n#  has no attribute 'LinearLR'\"\n!pip uninstall -y torch torchvision torchaudio torchtext\n!pip install torch torchvision pycocotools \n\nimport json\nimport os\nfrom pathlib import Path\nimport glob\nfrom itertools import product\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nfrom torch.nn import functional\nfrom PIL import Image, ImageEnhance\nimport pandas as pd\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom engine import train_one_epoch, evaluate\nimport utils\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport shutil\nfrom skimage.util import random_noise\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-02T07:03:19.92937Z","iopub.execute_input":"2022-06-02T07:03:19.929842Z","iopub.status.idle":"2022-06-02T07:05:30.288063Z","shell.execute_reply.started":"2022-06-02T07:03:19.929755Z","shell.execute_reply":"2022-06-02T07:05:30.287005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RSDDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset_dir, sec_dir, imgs_info, dataset_type):\n        self.dataset_dir = dataset_dir.joinpath('rtsd-frames')\n        self.imgs = imgs_info['file_name'].apply(\n            lambda x: (\n                str(self.dataset_dir.joinpath(x)) if\n                x not in ('2.3.5.jpg', '3_33.jpg') else\n                str(sec_dir.joinpath(x))\n            )\n        ).tolist()\n        self.bboxes = imgs_info['bbox'].tolist()\n        self.images_w = imgs_info['width'].tolist()\n        self.images_h = imgs_info['height'].tolist()\n        self.y = imgs_info['category_id'].tolist()\n        self.y2 = imgs_info['is_sign'].tolist()\n        self.area = imgs_info['area'].tolist()\n        self.iscrowd = imgs_info['iscrowd'].tolist()\n\n    def __getitem__(self, idx):\n        img_path = self.imgs[idx]\n        img = Image.open(img_path).convert('RGB')\n        \n        w = torch.as_tensor(self.images_w[idx], dtype=torch.int16)\n        h = torch.as_tensor(self.images_h[idx], dtype=torch.int16)\n        boxes = torch.as_tensor(self.bboxes[idx], dtype=torch.float32)\n        area = torch.as_tensor(self.area[idx], dtype=torch.int64)\n        labels = torch.as_tensor(self.y2[idx], dtype=torch.int64)\n        image_id = torch.tensor([idx])\n        iscrowd = torch.as_tensor(self.iscrowd[idx], dtype=torch.int64)\n\n        target = {}\n        target['image_id'] = image_id\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['iscrowd'] = iscrowd\n        target['area'] = area\n        target['y'] = torch.tensor(self.y[idx])\n        \n        \n        return T.ToTensor()(img), target\n\n    def __len__(self):\n        return len(self.imgs)\n    \n\ndef get_df_from_json_path(json_path: Path) -> pd.DataFrame:\n    with open(str(json_path)) as json_file:\n        jsn = json.load(json_file)\n    df_imgs = pd.DataFrame(jsn['images'])\n    df_anns = pd.DataFrame(jsn['annotations'])\n    df_cats = pd.DataFrame(jsn['categories'])\n    df = df_anns.merge(\n        df_cats, left_on='category_id', \n        right_on='id', how='left'\n    )\n    df = df_imgs.merge(df, left_on='id', right_on='image_id', how='left')\n    correct_columns = (\n        list(df.columns[1:4].values) + \n        list(df.columns[6:10].values) +\n        list([df.columns[11]])\n    )\n    df = (\n        df[correct_columns]\n        .groupby(['width', 'height', 'file_name'])\n        .agg(list).reset_index()\n    )\n    df['bbox'] = df['bbox'].apply(\n        lambda x: list(map(lambda y: [\n            y[0], y[1], y[0]+y[2], y[1]+y[3]\n        ], x))\n    )\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:12:50.657215Z","iopub.execute_input":"2022-06-02T07:12:50.657547Z","iopub.status.idle":"2022-06-02T07:12:50.682193Z","shell.execute_reply.started":"2022-06-02T07:12:50.657513Z","shell.execute_reply":"2022-06-02T07:12:50.681285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_DIR = (Path.cwd()\n               .parents[0]\n               .joinpath('input')\n               .joinpath('rtsd-dataset'))\n\nSECOND_DATASET_DIR = (\n    Path.cwd().parents[0]\n    .joinpath('input').joinpath('augmented-rsdd')\n)\n\ntrain_df = get_df_from_json_path(DATASET_DIR.joinpath('train_anno.json'))\ntest_df = get_df_from_json_path(DATASET_DIR.joinpath('val_anno.json'))\ndf = pd.concat([train_df, test_df])\n\n# add samples to classes with only instance\nrow = {\n    'width': 800, 'height': 400, \n    'file_name': '2.3.5.jpg', 'category_id': [141], \n    'area':[(745-675)*(210-135)], 'bbox': [[675, 135, 745, 210]],\n    'iscrowd': [0], 'name': ['2_3_5']\n      }\nrow2 = {\n    'width': 2974, 'height': 1576, \n    'file_name': '3_33.jpg', 'category_id': [76],\n    'area':[(2595-1795)*(905-95)], 'bbox': [[1795, 95, 2595, 905]],\n    'iscrowd': [0], 'name': ['3_33']\n       }\ndf = df.append(row, ignore_index = True)\ndf = df.append(row2, ignore_index = True)\n\n# add target for detector\ndf['is_sign'] = df['category_id'].apply(lambda x: [1]*len(x))\n\ntrain_df, test_df = train_test_split(df, test_size=0.1)\n    \ntrain_dataset = RSDDataset(DATASET_DIR, SECOND_DATASET_DIR, train_df, 'train')\ntest_dataset = RSDDataset(DATASET_DIR, SECOND_DATASET_DIR, test_df, 'test')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:12:54.510781Z","iopub.execute_input":"2022-06-02T07:12:54.51148Z","iopub.status.idle":"2022-06-02T07:13:00.653322Z","shell.execute_reply.started":"2022-06-02T07:12:54.511446Z","shell.execute_reply":"2022-06-02T07:13:00.652287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DETECTOR**","metadata":{}},{"cell_type":"code","source":"# detector train\nmodel = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n    pretrained=True\n)\nnum_classes = 2  # 1 class (sign) + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nfor p in model.roi_heads.box_predictor.parameters():\n    p.requires_grad = False\n\ndevice = (\n    torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n)\n\ndata_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=10, shuffle=True, num_workers=2,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    test_dataset, batch_size=4, shuffle=True, num_workers=2,\n    collate_fn=utils.collate_fn)\n\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params, lr=0.00015, weight_decay=0)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer, step_size=3, gamma=0.1\n)\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    metric_logger = train_one_epoch(\n        model, optimizer, data_loader, device, epoch, print_freq=1000\n    )\n    evaluate(model, data_loader_test, device=device)\n    lr_scheduler.step()\n\ntorch.save(model.state_dict(), 'detector_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:13:14.508764Z","iopub.execute_input":"2022-06-02T07:13:14.509072Z","iopub.status.idle":"2022-06-02T07:13:26.127562Z","shell.execute_reply.started":"2022-06-02T07:13:14.509028Z","shell.execute_reply":"2022-06-02T07:13:26.125935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_iou(bb1: list, bb2: list) -> float:\n    assert bb1[0] < bb1[2]\n    assert bb1[1] < bb1[3]\n    assert bb2[0] < bb2[2]\n    assert bb2[1] < bb2[3]\n    \n    x_left = max(bb1[0], bb2[0])\n    y_top = max(bb1[1], bb2[1])\n    x_right = min(bb1[2], bb2[2])\n    y_bottom = min(bb1[3], bb2[3])\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n    \n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n    \n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n    assert iou >= 0.0\n    assert iou <= 1.0\n    return iou\n\n\ndef calculate_iou_and_get_boxes(gt_boxes: list, pred_boxes: list) -> tuple:\n    # find intersected gt-boxes and pred-boxes\n    is_intersect = list(map(lambda x: list([\n        not (\n            gt_boxes[x][2] < pred_boxes[y][0] or\n            gt_boxes[x][0] > pred_boxes[y][2] or\n            gt_boxes[x][1] > pred_boxes[y][3] or\n            gt_boxes[x][3] < pred_boxes[y][1]\n        ) for y in range(len(pred_boxes))\n    ]), range(len(gt_boxes))))\n    \n    # find zero class boxes (without any part of signs)\n    #  to generate 156 class (no sign) in classifier\n    zero_class_boxes = np.array(is_intersect, dtype='int').sum(axis=0)\n    zero_class_boxes = np.where(zero_class_boxes == 0)[0]\n    zero_class_boxes = list(map(lambda x: pred_boxes[x], zero_class_boxes))\n    \n    # calculate iou for intersected boxes\n    for i in range(len(gt_boxes)):\n        for j in range(len(pred_boxes)):\n            is_intersect[i][j] = (\n                (get_iou(gt_boxes[i], pred_boxes[j]), j) if\n                is_intersect[i][j] else (0, j)\n            )\n        is_intersect[i] = sorted(\n            is_intersect[i], key=lambda x: x[0], reverse=True\n        )\n    \n    # find by iou best pred boxes for each gt box\n    used_pred_bboxes = []\n    ious = []\n    for row in is_intersect:\n        iou = 0\n        for row_idx in range(len(row)):\n            iou, pred_box_index = row[row_idx]\n            if iou == 0:\n                break\n            if pred_box_index not in used_pred_bboxes:\n                used_pred_bboxes.append(row_idx)\n                break\n        ious.append(iou)\n    return ious, zero_class_boxes\n\n\n# get: \n# 1. detector iou;\n# 2. zero class crops from test dataset for classifier learning\n!mkdir foldder\ncommon_iou = 0\nto_pil = T.ToPILImage()\nmodel.eval()\nfor i in tqdm(range(len(test_dataset))):\n    x, gt_boxes = test_dataset[i]\n    x = torch.unsqueeze(x, 0).to(device)\n    pred = model(x)\n    pred_boxes = pred[0]['boxes'].tolist()\n    gt_boxes = gt_boxes['boxes'].tolist()\n    boxes_iou, zero_boxes = calculate_iou_and_get_boxes(gt_boxes, pred_boxes)\n    if zero_boxes:\n        zero_boxes = list(map(lambda x: list(map(int, x)), zero_boxes))\n        for j, box in enumerate(zero_boxes):\n            # cut crop\n            crop = (\n                x[:, :, box[0]:box[2], box[1]:box[3]] if\n                box[2] <= x.shape[2] else\n                x[:, :, box[1]:box[3], box[0]:box[2]]\n            )\n            crop = torch.squeeze(crop, 0)\n            filename = (\n                './foldder/156_' +str(i).rjust(5, '0') + f'_{j}' + '.jpg'\n            )\n            to_pil(crop).save(\n                filename, \"JPEG\", quality=100, optimize=True, progressive=True\n            )\n    common_iou = (\n        (common_iou + (sum(boxes_iou) / len(boxes_iou))) / 2 if\n        common_iou != 0 else \n        (sum(boxes_iou) / len(boxes_iou)) / 2\n    )\n\n# make archive for easy download\nshutil.make_archive('zipped', 'zip', './foldder')\n    \nprint(common_iou)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:15:03.679608Z","iopub.execute_input":"2022-06-02T07:15:03.67995Z","iopub.status.idle":"2022-06-02T07:15:20.172033Z","shell.execute_reply.started":"2022-06-02T07:15:03.679904Z","shell.execute_reply":"2022-06-02T07:15:20.170196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get gt crops with labels from train dataset for classifier learning\n!mkdir foldder\nfor i in tqdm(range(len(train_dataset))):\n    x, gt_info = train_dataset[i]\n    labels = gt_info['y']\n    gt_boxes = gt_info['boxes'].tolist()\n    for j, box in enumerate(gt_boxes):\n        box = list(map(lambda x: int(x), box))\n        crop = (\n            x[:, box[0]:box[2], box[1]:box[3]] if\n            box[2] <= x.shape[1] else\n            x[:, box[1]:box[3], box[0]:box[2]]\n        )\n        file_name = (\n            f'./foldder/{labels[j]}_' +\n            str(i).rjust(5, '0') + f'_{j}' + '.jpg'\n        )\n        to_pil(crop).save(\n            file_name, \"JPEG\", quality=100,\n            optimize=True, progressive=True\n        )\n        \nshutil.make_archive('zipped', 'zip', './foldder')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:15:26.198698Z","iopub.execute_input":"2022-06-02T07:15:26.199616Z","iopub.status.idle":"2022-06-02T07:15:32.652185Z","shell.execute_reply.started":"2022-06-02T07:15:26.199563Z","shell.execute_reply":"2022-06-02T07:15:32.65041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CLASSIFIER**","metadata":{}},{"cell_type":"code","source":"zero_class_dataset_dir = SECOND_DATASET_DIR.joinpath('zipped-2')\nzero_files = glob.glob(str(zero_class_dataset_dir)+'/*')\nzero_files = list(map(lambda x: (x, 0), zero_files))\n\nclass_dataset_dir = SECOND_DATASET_DIR.joinpath('zipped-3')\nfiles = glob.glob(str(class_dataset_dir)+'/*')\nfiles = list(map(lambda x: (x, int(x.split('/')[-1].split('_')[0])), files))\nfiles.extend(zero_files)\ndf = pd.DataFrame(files, columns=['image', 'target'])\n\n# stratified train test split\ntrain_df = None\ntest_df = None\nused_indices = []\nfor t in df['target'].unique():\n    indices = list(df[df['target'] == t].index)\n    indices = list(filter(lambda x: x not in used_indices, indices))\n    l = len(indices)\n    l1 = int(l*0.9)\n    if train_df is None:\n        used_indices = indices\n        train_df = df.loc[indices[:l1]]\n        test_df = df.loc[indices[l1:]]\n    else:\n        used_indices.extend(indices)\n        train_df = pd.concat([train_df, df.loc[indices[:l1]]])\n        test_df = pd.concat([test_df, df.loc[indices[l1:]]])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:15:39.143735Z","iopub.execute_input":"2022-06-02T07:15:39.144437Z","iopub.status.idle":"2022-06-02T07:16:31.123043Z","shell.execute_reply.started":"2022-06-02T07:15:39.1444Z","shell.execute_reply":"2022-06-02T07:16:31.122049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def assign_augs(df: pd.DataFrame, suffs: list) -> pd.DataFrame:\n    target_counts = df.groupby('target').count()\n    to_aug_indices = target_counts[target_counts['image'] < 500].index.values\n    to_aug_samples = df[df['target'].isin(to_aug_indices)].copy()\n    \n    no_aug_indices = target_counts[~(target_counts['image'] < 500)].index.values\n    no_aug_samples = df[df['target'].isin(no_aug_indices)].copy()\n    \n    to_aug_samples['image'] = to_aug_samples['image'].apply(\n        lambda x: [x.replace('.jpg', s) for s in suffs]\n    )\n    augmented = to_aug_samples.explode('image').reset_index(drop=True)\n    df = pd.concat([augmented, no_aug_samples]).reset_index(drop=True)\n    return df\n    \nindices = list(map(str, list(range(7))))\nsuffixes = list(product(indices, indices, indices))\nsuffixes = list(map(lambda x: f'___{x[0]}___{x[1]}___{x[2]}.jpg', suffixes))\n\n# assign augs to classes with number of instances less than 500\ntrain_df = assign_augs(train_df, suffixes)\ntest_df = assign_augs(test_df, suffixes)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:17:45.456903Z","iopub.execute_input":"2022-06-02T07:17:45.457221Z","iopub.status.idle":"2022-06-02T07:17:50.674562Z","shell.execute_reply.started":"2022-06-02T07:17:45.45719Z","shell.execute_reply":"2022-06-02T07:17:50.673551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augs(filename: str) -> torch.Tensor:\n    levels_of_bright = [1, 0.3, 0.5, 0.7, 1.3, 1.5, 1.7]\n    levels_of_noise = [0, 0.001, 0.005, 0.01, 0.015, 0.02, 0.03]\n    levels_of_rotation = [0, 10, 6, 3, -3, -6, -10]\n    to_tensor = T.ToTensor()\n    if '___' in filename:\n        orig_img_path, b, n, r = filename.split('___')\n        b, n, r = int(b), int(n), int(r[:-4])\n        img = Image.open(orig_img_path + '.jpg')\n        img = (\n            ImageEnhance.Brightness(img).enhance(levels_of_bright[b])\n        )\n        img = to_tensor(img)\n        img = torch.tensor(random_noise(\n            img, mode='gaussian', mean=0,\n            var=levels_of_noise[n], clip=True\n        ))\n        img = T.functional.rotate(img, levels_of_rotation[r])\n    else:\n        img = Image.open(filename)\n        img = to_tensor(img)\n    return img\n\ndef balance_classes(df: pd.DataFrame) -> pd.DataFrame:\n    to_drop = []\n    list(map(\n        lambda t: (\n            to_drop.extend(\n                df[df['target'] == t]\n                .sample(len(df[df['target'] == t]) - 300)\n                .index.tolist()\n            )\n        ),\n        df['target'].unique()\n    ))\n    return df.drop(to_drop)\n\n# remain 300 instances for each class\ntrain_df = balance_classes(train_df)\ntest_df = balance_classes(test_df)\n\n# apply assigned augs\ntrain_df['image'] = train_df['image'].apply(augs)\ntest_df['image'] = test_df['image'].apply(augs)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:18:49.792997Z","iopub.execute_input":"2022-06-02T07:18:49.793275Z","iopub.status.idle":"2022-06-02T07:25:25.53682Z","shell.execute_reply.started":"2022-06-02T07:18:49.793243Z","shell.execute_reply":"2022-06-02T07:25:25.535801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassifierDataset(torch.utils.data.Dataset):\n    def __init__(self, imgs_info):\n        self.imgs = imgs_info['image'].tolist()\n        self.y = imgs_info['target'].tolist()\n\n    def __getitem__(self, idx):\n        to_pil = T.ToPILImage()\n        to_tensor = T.ToTensor()\n    \n        img = to_pil(self.imgs[idx]).resize((299, 299))\n        img = to_tensor(img).float()\n        \n        target = torch.tensor(self.y[idx])\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef train(\n    n_epochs, train_data, val_data, model,\n    loss_func, optimizer, dvc, bs, num_classes, scheduler\n):\n    for epoch in range(n_epochs):\n        train_loader = DataLoader(\n            dataset=train_data,\n            batch_size=bs,\n            shuffle=True,\n            drop_last=True\n        )\n        val_loader = DataLoader(\n            dataset=val_data,\n            batch_size=bs,\n            shuffle=True,\n            drop_last=True\n        )\n\n        loss_accum = 0\n        train_f1_accum = 0\n        i_step = 0\n        for i_step, batch in tqdm(enumerate(train_loader)):\n            model.train()\n            data = batch[0].to(dvc)\n            trg = batch[1].to(dvc)\n            pred = model(data).logits\n            loss = loss_func(pred.view(-1, num_classes), trg.view(-1))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            loss_accum += loss\n            train_f1_accum += f1_score(\n                trg.view(-1).cpu(), \n                torch.max(pred, -1)[1].view(-1).cpu(),\n                average='macro'\n            )\n\n        ave_loss = loss_accum / (i_step+1)\n        train_f1 = train_f1_accum / (i_step+1)\n        val_f1 = compute_f1(model, val_loader, dvc)\n        scheduler.step(val_f1)\n\n        print(f'Ave loss: {ave_loss}, Train f1: {train_f1}, Val f1: {val_f1}')\n\n        \ndef compute_f1(model, loader, dvc):\n    model.eval()\n\n    f1_accum = 0\n    i_step = 0\n    for i_step, batch in tqdm(enumerate(loader)):\n        data = batch[0].to(dvc)\n        ground_truth = batch[1].to(dvc)\n        pred = model(data)\n        f1_accum += f1_score(\n            ground_truth.view(-1).cpu(),\n            torch.max(pred, -1)[1].view(-1).cpu(),\n            average='micro'\n        )\n    return f1_accum / (i_step+1)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:26:37.952537Z","iopub.execute_input":"2022-06-02T07:26:37.952845Z","iopub.status.idle":"2022-06-02T07:26:37.972275Z","shell.execute_reply.started":"2022-06-02T07:26:37.952814Z","shell.execute_reply":"2022-06-02T07:26:37.971217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = torchvision.models.inception_v3(pretrained=True)\nfor param in classifier.parameters():\n    param.requires_grad = False\nclassifier.fc = nn.Linear(2048, 156)\nfor param in classifier.fc.parameters():\n    param.requires_grad = True\n\nnum_epochs = 10\ntrain_set = ClassifierDataset(train_df)\nval_set = ClassifierDataset(test_df)\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.Adam(classifier.parameters(), 0.003)\ndevice = (\n    torch.device('cuda') if\n    torch.cuda.is_available() else\n    torch.device('cpu')\n)\nclassifier.to(device)\nbatch_size = 128\nn_classes = 156  # 155 sign + no sign\nlr_scheduler = ReduceLROnPlateau(\n    optim, patience=0, mode='max', factor=0.5,\n    verbose=True, threshold=0.01\n)\ntrain(num_epochs, train_set, val_set, classifier,\n      criterion, optim, device, batch_size, n_classes, lr_scheduler)\n\ntorch.save(classifier.state_dict(), 'classifier_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T07:26:42.813104Z","iopub.execute_input":"2022-06-02T07:26:42.813479Z","iopub.status.idle":"2022-06-02T07:26:49.61502Z","shell.execute_reply.started":"2022-06-02T07:26:42.813445Z","shell.execute_reply":"2022-06-02T07:26:49.613265Z"},"trusted":true},"execution_count":null,"outputs":[]}]}