{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from shutil import copyfile\ncopyfile(src=\"../input/augmented-rsdd/engine.py\", dst=\"../working/engine.py\")\ncopyfile(src=\"../input/augmented-rsdd/coco_eval.py\", dst=\"../working/coco_eval.py\")\ncopyfile(src=\"../input/augmented-rsdd/coco_utils.py\", dst=\"../working/coco_utils.py\")\ncopyfile(src=\"../input/augmented-rsdd/group_by_aspect_ratio.py\", dst=\"../working/group_by_aspect_ratio.py\")\ncopyfile(src=\"../input/augmented-rsdd/presets.py\", dst=\"../working/presets.py\")\ncopyfile(src=\"../input/augmented-rsdd/train.py\", dst=\"../working/train.py\")\ncopyfile(src=\"../input/augmented-rsdd/transforms.py\", dst=\"../working/transforms.py\")\ncopyfile(src=\"../input/augmented-rsdd/utils.py\", dst=\"../working/utils.py\")\n\n# reinstall to fix \"module 'torch.optim.lr_scheduler' has no attribute 'LinearLR'\"\n!pip uninstall -y torch torchvision torchaudio torchtext && pip install torch torchvision pycocotools \n\nimport json\nimport os\nfrom pathlib import Path\nimport glob\nfrom itertools import product\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nfrom PIL import Image, ImageEnhance\nimport pandas as pd\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom engine import train_one_epoch, evaluate\nimport utils\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport shutil\nfrom skimage.util import random_noise\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-30T10:13:01.157827Z","iopub.execute_input":"2022-05-30T10:13:01.158409Z","iopub.status.idle":"2022-05-30T10:15:13.285294Z","shell.execute_reply.started":"2022-05-30T10:13:01.158280Z","shell.execute_reply":"2022-05-30T10:15:13.284125Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class RSDDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset_dir, sec_dir, imgs_info, dataset_type):\n        self.dataset_dir = dataset_dir.joinpath('rtsd-frames')\n        self.imgs = imgs_info['file_name'].apply(\n            lambda x: (\n                str(self.dataset_dir.joinpath(x)) if\n                x not in ('2.3.5.jpg', '3_33.jpg') else\n                str(sec_dir.joinpath(x))\n            )\n        ).tolist()\n        self.bboxes = imgs_info['bbox'].tolist()\n        self.images_w = imgs_info['width'].tolist()\n        self.images_h = imgs_info['height'].tolist()\n        self.y = imgs_info['category_id'].tolist()\n        self.y2 = imgs_info['is_sign'].tolist()\n        self.area = imgs_info['area'].tolist()\n        self.iscrowd = imgs_info['iscrowd'].tolist()\n\n    def __getitem__(self, idx):\n        img_path = self.imgs[idx]\n        img = Image.open(img_path).convert('RGB')\n        \n        w = torch.as_tensor(self.images_w[idx], dtype=torch.int16)\n        h = torch.as_tensor(self.images_h[idx], dtype=torch.int16)\n        boxes = torch.as_tensor(self.bboxes[idx], dtype=torch.float32)\n        area = torch.as_tensor(self.area[idx], dtype=torch.int64)\n        labels = torch.as_tensor(self.y2[idx], dtype=torch.int64)\n        image_id = torch.tensor([idx])\n        iscrowd = torch.as_tensor(self.iscrowd[idx], dtype=torch.int64)\n\n        target = {}\n        target['image_id'] = image_id\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"iscrowd\"] = iscrowd\n        target[\"area\"] = area\n        target['y'] = torch.tensor(self.y[idx])\n        \n        \n        return T.ToTensor()(img), target\n\n    def __len__(self):\n        return len(self.imgs)\n    \n\ndef get_df_from_json_path(json_path: Path) -> pd.DataFrame:\n    with open(str(json_path)) as json_file:\n        jsn = json.load(json_file)\n    df_imgs = pd.DataFrame(jsn['images'])\n    df_anns = pd.DataFrame(jsn['annotations'])\n    df_cats = pd.DataFrame(jsn['categories'])\n    df = df_anns.merge(df_cats, \n                       left_on='category_id', \n                       right_on='id', \n                       how='left')\n    df = df_imgs.merge(df, left_on='id', right_on='image_id', how='left')\n    correct_columns = (\n        list(df.columns[1:4].values) + \n        list(df.columns[6:10].values) +\n        list([df.columns[11]])\n    )\n    df = (df[correct_columns]\n          .groupby(['width', 'height', 'file_name'])\n          .agg(list).reset_index())\n    df['bbox'] = df['bbox'].apply(\n        lambda x: list(map(lambda y: [y[0], \n                                      y[1], \n                                      y[0]+y[2], \n                                      y[1]+y[3]], x))\n    )\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-30T11:30:42.699507Z","iopub.execute_input":"2022-05-30T11:30:42.701105Z","iopub.status.idle":"2022-05-30T11:30:42.725294Z","shell.execute_reply.started":"2022-05-30T11:30:42.701028Z","shell.execute_reply":"2022-05-30T11:30:42.724060Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"DATASET_DIR = (Path.cwd()\n               .parents[0]\n               .joinpath('input')\n               .joinpath('rtsd-dataset'))\ntrain_imgs_info = get_df_from_json_path(DATASET_DIR.joinpath('train_anno.json'))\ntest_imgs_info = get_df_from_json_path(DATASET_DIR.joinpath('val_anno.json'))\ndf = pd.concat([train_imgs_info, test_imgs_info])\n\nrow = {\n    'width': 800, 'height': 400, \n    'file_name': '2.3.5.jpg', 'category_id': [141], \n    'area':[(745-675)*(210-135)], 'bbox': [[675, 135, 745, 210]],\n    'iscrowd': [0], 'name': ['2_3_5']\n      }\nrow2 = {\n    'width': 2974, 'height': 1576, \n    'file_name': '3_33.jpg', 'category_id': [76],\n    'area':[(2595-1795)*(905-95)], 'bbox': [[1795, 95, 2595, 905]],\n    'iscrowd': [0], 'name': ['3_33']\n       }\ndf = df.append(row, ignore_index = True)\ndf = df.append(row2, ignore_index = True)\n\ndf['cat_id_s'] = df['category_id'].apply(\n    lambda x: '_' + '_'.join(str(i) for i in x) + '_'\n)\n\ndf['is_sign'] = df['category_id'].apply(lambda x: [1]*len(x))\n\ntrain_imgs_info, test_imgs_info = train_test_split(df, test_size=0.1)\n\n# df['common_y'] = df['category_id'].apply(lambda x: x[0] if len(x) == 1 else -1)\n# df_to_aug = df[df['common_y'] != -1]\n# print(collections.Counter(df['category_id'].explode('category_id').values))\n# print(collections.Counter(df_to_aug['category_id'].explode('category_id').values))\n# assert 1==2\n\n\n# test_imgs_info = add_augs_to_df(test_imgs_info)\n\n# print(test_imgs_info['common_y'].value_counts())\n# assert 1==2\n\n\n# counts_id = test_imgs_info.explode('category_id')['category_id'].value_counts()\n\n# test_imgs_info['cat_id_s'] = test_imgs_info['category_id'].apply(\n#     lambda x: '_' + '_'.join(str(i) for i in x) + '_'\n# )\n\n# drop_indices = []\n# for cat in counts_id.index:\n#     counts = len(test_imgs_info[test_imgs_info['cat_id_s'].str.contains(f'_{cat}_')])\n#     drop_indices.extend(\n#         np.random.choice(\n#             test_imgs_info[test_imgs_info['cat_id_s'].str.contains(f'_{cat}_')].index,\n#             counts - 300,\n#             replace=False\n#         )\n#     )\n# test_imgs_info = test_imgs_info.drop(list(set(drop_indices)))\n# counts = test_imgs_info.explode('category_id')['category_id'].value_counts()\n# print(counts)\n\nsec_ds = (\n    Path.cwd().parents[0]\n    .joinpath('input').joinpath('augmented-rsdd')\n)\n    \ntrain_dataset = RSDDataset(DATASET_DIR, sec_ds, train_imgs_info, 'train')\ntest_dataset = RSDDataset(DATASET_DIR, sec_ds, test_imgs_info, 'test')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T11:30:43.065988Z","iopub.execute_input":"2022-05-30T11:30:43.066348Z","iopub.status.idle":"2022-05-30T11:30:48.857907Z","shell.execute_reply.started":"2022-05-30T11:30:43.066315Z","shell.execute_reply":"2022-05-30T11:30:48.857050Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**FASTER R-CNN**","metadata":{}},{"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 156  # 155 class (signs) + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndata_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=4, shuffle=True, num_workers=2,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    test_dataset, batch_size=1, shuffle=True, num_workers=2,\n    collate_fn=utils.collate_fn)\n\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params, lr=0.0001, weight_decay=0)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer, step_size=3, gamma=0.1\n)\n\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n    evaluate(model, data_loader_test, device=device)\n    lr_scheduler.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install onnx\nimport onnx\n\nmodel.eval()\ndevice = torch.device('cpu')\nmodel.to(device)\nx, target = train_dataset[0]\nx = torch.unsqueeze(x, 0)\ntorch_out = model(x)\ntorch.save(x, 'x.pt')\ntorch.save(torch_out, 'torch_out.pt')\n\ndynamic_axes = {'input': [0, 2, 3], 'output': [0, 2, 3]}\n\ntorch.onnx.export(model,\n                  x,\n                  \"faster_rcnn_79.onnx\",\n                  export_params=True,\n                  opset_version=14,\n                  do_constant_folding=True,\n                  dynamic_axes=dynamic_axes,\n                  input_names=['input'],\n                  output_names=['output'])\n\nonnx_model = onnx.load(\"faster_rcnn_79.onnx\")\nonnx.checker.check_model(onnx_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MOBILENET**","metadata":{}},{"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\nnum_classes = 2  # 1 class (sign) + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nfor p in model.roi_heads.box_predictor.parameters():\n    p.requires_grad = False\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndata_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=10, shuffle=True, num_workers=2,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    test_dataset, batch_size=4, shuffle=True, num_workers=2,\n    collate_fn=utils.collate_fn)\n\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params, lr=0.00015, weight_decay=0)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer, step_size=3, gamma=0.1\n)\n\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1000)\n    evaluate(model, data_loader_test, device=device)\n    lr_scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T07:58:21.151282Z","iopub.execute_input":"2022-05-26T07:58:21.151564Z","iopub.status.idle":"2022-05-26T12:35:14.811877Z","shell.execute_reply.started":"2022-05-26T07:58:21.151535Z","shell.execute_reply":"2022-05-26T12:35:14.81091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install onnx\nimport onnx\n\nmodel.eval()\ndevice = torch.device('cpu')\nmodel.to(device)\nx, target = train_dataset[0]\nx = torch.unsqueeze(x, 0)\ntorch_out = model(x)\ntorch.save(x, 'x_mobile5_detector.pt')\ntorch.save(torch_out, 'torch_out_mobile_detector.pt')\n\ntorch.onnx.export(model,\n                  x,\n                  \"faster_rcnn_73_mobile_detector.onnx\",\n                  export_params=True,\n                  opset_version=11)\n\nonnx_model = onnx.load(\"faster_rcnn_73_mobile_detector.onnx\")\nonnx.checker.check_model(onnx_model)\n\ntorch.save(model.state_dict(), 'model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T07:45:37.247531Z","iopub.execute_input":"2022-05-28T07:45:37.247839Z","iopub.status.idle":"2022-05-28T07:45:51.935056Z","shell.execute_reply.started":"2022-05-28T07:45:37.247759Z","shell.execute_reply":"2022-05-28T07:45:51.933591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_iou(bb1, bb2):\n    assert bb1[0] < bb1[2]\n    assert bb1[1] < bb1[3]\n    assert bb2[0] < bb2[2]\n    assert bb2[1] < bb2[3]\n    \n    x_left = max(bb1[0], bb2[0])\n    y_top = max(bb1[1], bb2[1])\n    x_right = min(bb1[2], bb2[2])\n    y_bottom = min(bb1[3], bb2[3])\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n    \n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n    \n    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n    \n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n    assert iou >= 0.0\n    assert iou <= 1.0\n    return iou\n\n\ndef calculate_iou(gt_boxes, pred_boxes):\n    is_intersect = list(map(lambda x: list([\n        not (gt_boxes[x][2] < pred_boxes[y][0] or\n             gt_boxes[x][0] > pred_boxes[y][2] or\n             gt_boxes[x][1] > pred_boxes[y][3] or\n             gt_boxes[x][3] < pred_boxes[y][1]) for y in range(len(pred_boxes))\n    ]), range(len(gt_boxes))))\n    null_class_boxes = np.array(is_intersect, dtype='int').sum(axis=0)\n    null_class_boxes = np.where(null_class_boxes == 0)[0]\n    null_class_boxes = list(map(lambda x: pred_boxes[x], null_class_boxes))\n    \n    for i in range(len(gt_boxes)):\n        for j in range(len(pred_boxes)):\n            is_intersect[i][j] = (get_iou(gt_boxes[i], pred_boxes[j]), j) if is_intersect[i][j] else (0, j)\n        is_intersect[i] = sorted(is_intersect[i], key=lambda x: x[0] if isinstance(x, tuple) else x, reverse=True)\n    \n    used_pred_bboxes = []\n    ious = []\n    for row in is_intersect:\n        iou = 0\n        for row_idx in range(len(row)):\n            iou, pred_box_index = row[row_idx]\n            if iou == 0:\n                break\n            if pred_box_index not in used_pred_bboxes:\n                used_pred_bboxes.append(row_idx)\n                break\n        ious.append(iou)\n    return ious, null_class_boxes\n\n!mkdir foldder\nmodel = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\nnum_classes = 2  # 1 class (sign) + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nfor p in model.parameters():\n    p.requires_grad = False\n\nmodel.load_state_dict(torch.load(str(sec_ds.joinpath('model_weights.pth'))))\nmodel.eval()\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\nintersection_over_union = 0\nfor i in tqdm(range(len(test_dataset))):\n    x, gt_boxes = test_dataset[i]\n    x = torch.unsqueeze(x, 0).to(device)\n    pred = model(x)\n    pred_boxes = pred[0]['boxes'].tolist()\n    boxes_iou, null_boxes = calculate_iou(gt_boxes['boxes'].tolist(), pred_boxes)\n    if null_boxes:\n        null_boxes = list(map(lambda x: list(map(int, x)), null_boxes))\n        for j, box in enumerate(null_boxes):\n            crop = (\n                x[:, :, box[0]:box[2], box[1]:box[3]] if\n                box[2] <= x.shape[2] else\n                x[:, :, box[1]:box[3], box[0]:box[2]]\n            )\n            file_name = './foldder/156_' + str(i).rjust(5, '0')+ f'_{j}' + '.jpg'\n            (\n                T.ToPILImage()(torch.squeeze(crop, 0))\n                .save(filename, \n                      \"JPEG\", \n                      quality=100, \n                      optimize=True, \n                      progressive=True)\n            )\n    intersection_over_union = (\n        (intersection_over_union + (sum(boxes_iou) / len(boxes_iou))) / 2 if\n        intersection_over_union != 0 else \n        (sum(boxes_iou) / len(boxes_iou)) / 2\n    )\n    \nshutil.make_archive('zipped', 'zip', './foldder')\n    \nprint('-'*100)\nprint(intersection_over_union)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T10:31:51.338479Z","iopub.execute_input":"2022-05-28T10:31:51.338759Z","iopub.status.idle":"2022-05-28T10:38:10.911582Z","shell.execute_reply.started":"2022-05-28T10:31:51.338726Z","shell.execute_reply":"2022-05-28T10:38:10.91071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!mkdir foldder\nfor i in tqdm(range(len(train_dataset))):\n    x, gt_info = train_dataset[i]\n    print(gt_info)\n    labels = gt_info['y']\n    gt_boxes = gt_info['boxes'].tolist()\n    for j, box in enumerate(gt_boxes):\n        box = list(map(lambda x: int(x), box))\n        crop = (\n            x[:, box[0]:box[2], box[1]:box[3]] if\n            box[2] <= x.shape[1] else\n            x[:, box[1]:box[3], box[0]:box[2]]\n        )\n        file_name = (\n            f'./foldder/{labels[j]}_' +\n            str(i).rjust(5, '0') +\n            f'_{j}' +\n            '.jpg')\n        print(file_name)\n        assert 1==2\n        T.ToPILImage()(crop).save(file_name, \n                                  \"JPEG\", \n                                  quality=100, \n                                  optimize=True, \n                                  progressive=True)\nshutil.make_archive('zipped', 'zip', './foldder')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T11:31:21.664652Z","iopub.execute_input":"2022-05-30T11:31:21.665793Z","iopub.status.idle":"2022-05-30T11:31:21.794933Z","shell.execute_reply.started":"2022-05-30T11:31:21.665697Z","shell.execute_reply":"2022-05-30T11:31:21.794025Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**CLASSIFIER**","metadata":{}},{"cell_type":"code","source":"null_class_dataset_dir = sec_ds.joinpath('zipped-2')\nclass_dataset_dir = sec_ds.joinpath('zipped-3')\nnull_files = glob.glob(str(null_class_dataset_dir)+'/*')\nfiles = glob.glob(str(class_dataset_dir)+'/*')\nnull_files = list(map(lambda x: (x, 0), null_files))\nfiles = list(map(lambda x: (x, int(x.split('/')[-1].split('_')[0])), files))\nfiles.extend(null_files)\ndf = pd.DataFrame(files, columns=['image', 'target'])\n\n# train test split 0.9\ntrain_imgs_info = None\ntest_imgs_info = None\nused_indices = []\nfor t in df['target'].unique():\n    indices = list(df[df['target'] == t].index)\n    indices = list(filter(lambda x: x not in used_indices, indices))\n    l = len(indices)\n    l1 = int(l*0.9)\n    if train_imgs_info is None:\n        used_indices = indices\n        train_imgs_info = df.loc[indices[:l1]]\n        test_imgs_info = df.loc[indices[l1:]]\n    else:\n        used_indices.extend(indices)\n        train_imgs_info = pd.concat([train_imgs_info, df.loc[indices[:l1]]])\n        test_imgs_info = pd.concat([test_imgs_info, df.loc[indices[l1:]]])\nprint(train_imgs_info['target'].value_counts())\nprint(test_imgs_info['target'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:48:23.161918Z","iopub.execute_input":"2022-05-30T04:48:23.162287Z","iopub.status.idle":"2022-05-30T04:49:10.662131Z","shell.execute_reply.started":"2022-05-30T04:48:23.162248Z","shell.execute_reply":"2022-05-30T04:49:10.661425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_augs_to_df(df, suff):\n    df['image'] = df['image'].apply(\n        lambda x: [x.replace('.jpg', s) for s in suff]\n    )\n    return df.explode('image').reset_index(drop=True)\n\n\nindices = list(map(str, list(range(7))))\nsuffixes = list(product(indices, indices, indices))\nsuffixes = list(map(lambda x: f'___{x[0]}___{x[1]}___{x[2]}.jpg', suffixes))\n\n#добавляю инфу о аугментациях таргетов, кол-во семплов которых не превосходит 500 штук.\ntrain_target_counts = train_imgs_info.groupby('target').count()\ntrain_to_aug = train_target_counts[train_target_counts['image'] < 500].index.values\ntrain_to_aug = train_imgs_info[train_imgs_info['target'].isin(train_to_aug)].copy()\ntrain_no_aug = train_target_counts[~(train_target_counts['image'] < 500)].index.values\ntrain_no_aug = train_imgs_info[train_imgs_info['target'].isin(train_no_aug)].copy()\naugmented = add_augs_to_df(train_to_aug, suffixes)\ntrain_imgs_info = pd.concat([augmented, train_no_aug]).reset_index(drop=True)\n\ntest_target_counts = test_imgs_info.groupby('target').count()\ntest_to_aug = test_target_counts[test_target_counts['image'] < 500].index.values\ntest_to_aug = test_imgs_info[test_imgs_info['target'].isin(test_to_aug)].copy()\ntest_no_aug = test_target_counts[~(test_target_counts['image'] < 500)].index.values\ntest_no_aug = test_imgs_info[test_imgs_info['target'].isin(test_no_aug)].copy()\naugmented = add_augs_to_df(test_to_aug, suffixes)\ntest_imgs_info = pd.concat([augmented, test_no_aug]).reset_index(drop=True)\n\nprint(train_imgs_info['target'].value_counts())\nprint(test_imgs_info['target'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:49:10.663912Z","iopub.execute_input":"2022-05-30T04:49:10.664164Z","iopub.status.idle":"2022-05-30T04:49:15.03776Z","shell.execute_reply.started":"2022-05-30T04:49:10.664128Z","shell.execute_reply":"2022-05-30T04:49:15.037001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augs(filename):\n    levels_of_bright = [1, 0.3, 0.5, 0.7, 1.3, 1.5, 1.7]\n    levels_of_noise = [0, 0.001, 0.005, 0.01, 0.015, 0.02, 0.03]\n    levels_of_rotation = [0, 10, 6, 3, -3, -6, -10]\n    if '___' in filename:\n        orig_img_path, b, n, r = filename.split('___')\n        b, n, r = int(b), int(n), int(r[:-4])\n        img = Image.open(orig_img_path + '.jpg')\n        img = ImageEnhance.Brightness(img).enhance(levels_of_bright[b])\n        img = T.ToTensor()(img)\n        img = torch.tensor(random_noise(img, mode='gaussian', mean=0, var=levels_of_noise[n], clip=True))\n        img = T.functional.rotate(img, levels_of_rotation[r])\n    else:\n        img = Image.open(filename)\n        img = T.ToTensor()(img)\n    return img\n\n# оставляю в датафреймах по 300 штук каждого класса\ntrain_to_drop = []\nlist(map(\n    lambda t: (\n        train_to_drop.extend(\n            train_imgs_info[train_imgs_info['target'] == t]\n            .sample(len(train_imgs_info[train_imgs_info['target'] == t]) - 300)\n            .index.tolist()\n        )\n    ),\n    train_imgs_info['target'].unique()\n))\ntrain_imgs_info = train_imgs_info.drop(train_to_drop)\n\ntest_to_drop = []\nlist(map(\n    lambda t: (\n        test_to_drop.extend(\n            test_imgs_info[test_imgs_info['target'] == t]\n            .sample(len(test_imgs_info[test_imgs_info['target'] == t]) - 300)\n            .index.tolist()\n        )\n    ),\n    test_imgs_info['target'].unique()\n))\ntest_imgs_info = test_imgs_info.drop(test_to_drop)\n\nprint(train_imgs_info['target'].value_counts())\nprint(test_imgs_info['target'].value_counts())\n\n\n#провожу часть аугментаций и загружаю тензоры в оперативку\ntrain_imgs_info['image'] = train_imgs_info['image'].apply(augs)\ntest_imgs_info['image'] = test_imgs_info['image'].apply(augs)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:49:15.039137Z","iopub.execute_input":"2022-05-30T04:49:15.039547Z","iopub.status.idle":"2022-05-30T04:55:00.413325Z","shell.execute_reply.started":"2022-05-30T04:49:15.039509Z","shell.execute_reply":"2022-05-30T04:55:00.412603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassifierDataset(torch.utils.data.Dataset):\n    def __init__(self, imgs_info):\n        self.imgs = imgs_info['image'].tolist()\n        self.y = imgs_info['target'].tolist()\n\n    def __getitem__(self, idx):\n        # провожу оставшуюся аугментацию (ресайз)\n        return T.ToTensor()(T.ToPILImage()(self.imgs[idx]).resize((299, 299))).float(), torch.tensor(self.y[idx])\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef train(n_epochs, train_data, val_data, model,\n          loss_func, optimizer, dvc, bs, n_breeds, scheduler):\n    for epoch in range(n_epochs):\n        train_loader = DataLoader(\n            dataset=train_data,\n            #collate_fn=collate_fn,\n            batch_size=bs,\n            shuffle=True,\n            drop_last=True\n        )\n        val_loader = DataLoader(\n            dataset=val_data,\n            #collate_fn=collate_fn,\n            batch_size=bs,\n            shuffle=True,\n            drop_last=True\n        )\n\n        loss_accum = 0\n        train_f1_accum = 0\n        i_step = 0\n        for i_step, batch in tqdm(enumerate(train_loader)):\n            model.train()\n            data = batch[0].to(dvc)\n            trg = batch[1].to(dvc)\n            pred = model(data).logits\n            loss = loss_func(pred.view(-1, n_breeds), trg.view(-1))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            loss_accum += loss\n            train_f1_accum += f1_score(trg.view(-1).cpu(),\n                                       torch.max(pred, -1)[1].view(-1).cpu(),\n                                       average='macro')\n\n        ave_loss = loss_accum / (i_step+1)\n        train_f1 = train_f1_accum / (i_step+1)\n        val_f1 = compute_f1(model, val_loader, dvc)\n        scheduler.step(val_f1)\n\n        print(f'Ave loss: {ave_loss}, Train f1: {train_f1}, Val f1: {val_f1}')\n        \n        \ndef compute_f1(model, loader, dvc):\n    model.eval()\n\n    f1_accum = 0\n    i_step = 0\n    for i_step, batch in tqdm(enumerate(loader)):\n        data = batch[0].to(dvc)\n        ground_truth = batch[1].to(dvc)\n        pred = model(data)\n        f1_accum += f1_score(ground_truth.view(-1).cpu(),\n                             torch.max(pred, -1)[1].view(-1).cpu(),\n                             average='macro')\n    return f1_accum / (i_step+1)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:01:13.655263Z","iopub.execute_input":"2022-05-30T05:01:13.655523Z","iopub.status.idle":"2022-05-30T05:01:13.67238Z","shell.execute_reply.started":"2022-05-30T05:01:13.655494Z","shell.execute_reply":"2022-05-30T05:01:13.671383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = torchvision.models.inception_v3(pretrained=True)\nfor param in classifier.parameters():\n    param.requires_grad = False\nclassifier.fc = nn.Linear(2048, 156)\nfor param in classifier.fc.parameters():\n    param.requires_grad = True\n\nnum_epochs = 10\ntrain_set = ClassifierDataset(train_imgs_info)\nval_set = ClassifierDataset(test_imgs_info)\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.Adam(classifier.parameters(), 0.003)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nclassifier.to(device)\nbatch_size = 128\nn_classes = 156\nlr_scheduler = ReduceLROnPlateau(optim,\n                                 patience=0,\n                                 mode='max',\n                                 factor=0.5,\n                                 verbose=True,\n                                 threshold=0.01)\ntrain(num_epochs, train_set, val_set, classifier,\n      criterion, optim, device, batch_size, n_classes, lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:01:32.838457Z","iopub.execute_input":"2022-05-30T05:01:32.839158Z","iopub.status.idle":"2022-05-30T06:25:37.782075Z","shell.execute_reply.started":"2022-05-30T05:01:32.839118Z","shell.execute_reply":"2022-05-30T06:25:37.780405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(classifier.state_dict(), 'classifier_weights.pth')\n\n# def adjust_brightness(image)\n#     new_images = []\n#     factors = [1, 0.3, 0.5, 0.7, 1.3, 1.5, 1.7]\n#     for factor in factors:\n#         new_images.extend(list(map(lambda x: (\n#             ImageEnhance.Brightness(x).enhance(factor)\n#         ), image)))\n#     images.extend(new_images)\n    \n# def adjust_noise(images: list, bboxes: list, augs: list)\n#     new_images = []\n#     for var in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]:\n#         new_images.extend(list(map(lambda x: (\n#             torch.tensor(random_noise(img, mode='gaussian', mean=0, var=var, clip=True))\n#         ), images)))\n#     images.extend(new_images)\n#     bboxes.extend(bboxes*7)\n    \n    \n# def adjust_angle(images: list, bboxes: list, augs: list):\n#     for angle in [0, 3, 6, 10, -3, -6, -10]:\n#         bb = bboxes[idx][0]\n#         bb = np.array(((bb[0],bb[1]),(bb[2],bb[1]),(bb[2],bb[3]),(bb[0],bb[3])))\n#         center = (img.size[0]//2,img.size[1]//2)\n#         rotMat = cv2.getRotationMatrix2D(center,angle,1.0)\n#         img_rotated = T.functional.rotate(t_img, angle)\n#         bb_rotated = np.vstack((bb.T,np.array((1,1,1,1))))\n#         bb_rotated = np.dot(rotMat,bb_rotated).T\n#         images.append(img_rotated)\n#         bboxes.extend(bboxes*7)\n#     return images, bboxes","metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:29:40.048494Z","iopub.execute_input":"2022-05-30T06:29:40.049045Z","iopub.status.idle":"2022-05-30T06:29:40.274255Z","shell.execute_reply.started":"2022-05-30T06:29:40.049005Z","shell.execute_reply":"2022-05-30T06:29:40.273546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}